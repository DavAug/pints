{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference: No-U-Turn MCMC\n",
    "\n",
    "This example shows you how to perform Bayesian inference on a Gaussian distribution and a time-series problem, using [No-U-Turn Monte Carlo](http://pints.readthedocs.io/en/latest/mcmc_samplers/nuts_mcmc.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create a simple normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pints\n",
    "import pints.toy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create log pdf\n",
    "log_pdf = pints.toy.GaussianLogPDF([2, 4], [[1, 0], [0, 3]])\n",
    "\n",
    "# Contour plot of pdf\n",
    "levels = np.linspace(-3,12,20)\n",
    "num_points = 100\n",
    "x = np.linspace(-1, 5, num_points)\n",
    "y = np.linspace(-0, 8, num_points)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = np.zeros(X.shape)\n",
    "Z = np.exp([[log_pdf([i, j]) for i in x] for j in y])\n",
    "plt.contour(X, Y, Z)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we set up and run a sampling routine using Hamiltonian MCMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose starting points for 3 mcmc chains\n",
    "xs = [\n",
    "    [2, 1],\n",
    "    [3, 3],\n",
    "    [5, 4],\n",
    "]\n",
    "\n",
    "# Set a standard deviation, to give the method a sense of scale\n",
    "#sigma = [1, 1]\n",
    "\n",
    "# Create mcmc routine\n",
    "mcmc = pints.MCMCController(log_pdf, 3, xs, method=pints.NoUTurnMCMC)\n",
    "\n",
    "# Add stopping criterion\n",
    "mcmc.set_max_iterations(2000)\n",
    "\n",
    "# Set up modest logging\n",
    "mcmc.set_log_to_screen(True)\n",
    "mcmc.set_log_interval(100)\n",
    "\n",
    "# Run!\n",
    "print('Running...')\n",
    "full_chains = mcmc.run()\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show traces and histograms\n",
    "import pints.plot\n",
    "pints.plot.trace(full_chains)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discard warm up\n",
    "chains = full_chains[:, 200:]\n",
    "\n",
    "# Check convergence using rhat criterion\n",
    "print('R-hat:')\n",
    "print(pints.rhat_all_params(chains))\n",
    "\n",
    "# Check Kullback-Leibler divergence of chains\n",
    "print(log_pdf.kl_divergence(chains[0]))\n",
    "print(log_pdf.kl_divergence(chains[1]))\n",
    "print(log_pdf.kl_divergence(chains[2]))\n",
    "\n",
    "# Look at distribution in chain 0\n",
    "pints.plot.pairwise(chains[0], kde=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No-U-Turn MCMC on a time-series problem\n",
    "\n",
    "We now try the same method on a time-series problem\n",
    "\n",
    "First, we try it in 1d, using a wrapper around the LogisticModel to make it one-dimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pints.toy as toy\n",
    "\n",
    "# Create a wrapper around the logistic model, turning it into a 1d model\n",
    "class Model(pints.ForwardModel):\n",
    "    def __init__(self):\n",
    "        self.model = toy.LogisticModel()\n",
    "    def simulate(self, x, times):\n",
    "        return self.model.simulate([x[0], 500], times)\n",
    "    def simulateS1(self, x, times):\n",
    "        values, gradient = self.model.simulateS1([x[0], 500], times)\n",
    "        gradient = gradient[:, 0]\n",
    "        return values, gradient\n",
    "    def n_parameters(self):\n",
    "        return 1\n",
    "\n",
    "# Load a forward model\n",
    "model = Model()\n",
    "    \n",
    "# Create some toy data\n",
    "real_parameters = np.array([0.015])\n",
    "times = np.linspace(0, 1000, 50)\n",
    "org_values = model.simulate(real_parameters, times)\n",
    "\n",
    "# Add noise\n",
    "np.random.seed(1)\n",
    "noise = 10\n",
    "values = org_values + np.random.normal(0, noise, org_values.shape)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(times, values)\n",
    "plt.plot(times, org_values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use optimisation to find the parameter value that maximises the loglikelihood, and note that it's become slightly biased due to noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an object with links to the model and time series\n",
    "problem = pints.SingleOutputProblem(model, times, values)\n",
    "\n",
    "# Create a log-likelihood function\n",
    "log_likelihood = pints.GaussianKnownSigmaLogLikelihood(problem, noise)\n",
    "\n",
    "# Find the best parameters with XNES\n",
    "best_parameters, fx = pints.optimise(log_likelihood, real_parameters, method=pints.XNES)\n",
    "print(best_parameters[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the likelihood near the true parameters\n",
    "plt.figure()\n",
    "x = np.linspace(0.01497, 0.01505, 500)\n",
    "y = [log_likelihood([i]) for i in x]\n",
    "plt.axvline(real_parameters[0], color='tab:orange', label='real')\n",
    "plt.axvline(best_parameters[0], color='tab:green', label='found')\n",
    "plt.legend()\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the LogisticModel (and our wrapper) support the `evaluatS1()` method, we can also evaluate the gradient of the loglikelihood at different points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show derivatives at two points\n",
    "y1, dy1 = log_likelihood.evaluateS1(real_parameters)\n",
    "y2, dy2 = log_likelihood.evaluateS1(best_parameters)\n",
    "\n",
    "# Show the likelihood near the true parameters\n",
    "x = np.linspace(0.01498, 0.01502, 500)\n",
    "y = [log_likelihood([i]) for i in x]\n",
    "z1 = y1 + (x - real_parameters[0]) * dy1\n",
    "z2 = y2 + (x - best_parameters[0]) * dy2\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x, y)\n",
    "plt.plot(x, z1, label='real parameters')\n",
    "plt.plot(x, z2, label='found parameters')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Satisfied that this works, we now run a NoUTurnMCMC routine (which uses the derivative information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose starting points for mcmc chains\n",
    "xs = [\n",
    "    real_parameters * 1.01,\n",
    "    real_parameters * 0.9,\n",
    "    real_parameters * 1.15,\n",
    "]\n",
    "\n",
    "# Choose a covariance matrix for the proposal step\n",
    "#sigma0 = (best_parameters - real_parameters) * 0.1\n",
    "sigma0 = np.abs(real_parameters)\n",
    "\n",
    "# Create a uniform prior over both the parameters and the new noise variable\n",
    "log_prior = pints.UniformLogPrior(\n",
    "    [0.01],\n",
    "    [0.02]\n",
    ")\n",
    "\n",
    "# Make posterior\n",
    "log_posterior = pints.LogPosterior(log_likelihood, log_prior)\n",
    "\n",
    "# Create mcmc routine\n",
    "mcmc = pints.MCMCController(log_posterior, len(xs), xs, method=pints.NoUTurnMCMC)\n",
    "\n",
    "# Add stopping criterion\n",
    "mcmc.set_max_iterations(1000)\n",
    "\n",
    "# Set up modest logging\n",
    "mcmc.set_log_to_screen(True)\n",
    "mcmc.set_log_interval(100)\n",
    "\n",
    "# Set small step size\n",
    "# for sampler in mcmc.samplers():\n",
    "#     sampler.set_leapfrog_step_size(3e-5)   # This is very sensitive!\n",
    "\n",
    "# Run!\n",
    "print('Running...')\n",
    "chains = mcmc.run()\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show trace and histogram\n",
    "pints.plot.trace(chains)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show predicted time series for the first chain\n",
    "pints.plot.series(chains[0, 200:], problem, real_parameters)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2d Time series\n",
    "\n",
    "Finally, we try No-U-Turn MCMC on a 2d logistic model problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pints\n",
    "import pints.toy as toy\n",
    "import pints.plot\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load a forward model\n",
    "model = toy.LogisticModel()\n",
    "\n",
    "# Create some toy data\n",
    "real_parameters = np.array([0.015, 500])\n",
    "org_values = model.simulate(real_parameters, times)\n",
    "\n",
    "# Add noise\n",
    "np.random.seed(1)\n",
    "noise = 10\n",
    "values = org_values + np.random.normal(0, noise, org_values.shape)\n",
    "\n",
    "# Create an object with links to the model and time series\n",
    "problem = pints.SingleOutputProblem(model, times, values)\n",
    "\n",
    "# Create a log-likelihood function\n",
    "log_likelihood = pints.GaussianKnownSigmaLogLikelihood(problem, noise)\n",
    "\n",
    "# Create a uniform prior over the parameters\n",
    "log_prior = pints.UniformLogPrior(\n",
    "    [0.01, 400],\n",
    "    [0.02, 600]\n",
    ")\n",
    "\n",
    "# Create a posterior log-likelihood (log(likelihood * prior))\n",
    "log_posterior = pints.LogPosterior(log_likelihood, log_prior)\n",
    "\n",
    "# Choose starting points for 3 mcmc chains\n",
    "xs = [\n",
    "    real_parameters * 1.01,\n",
    "    real_parameters * 0.9,\n",
    "    real_parameters * 1.1,\n",
    "]\n",
    "\n",
    "# Create mcmc routine\n",
    "mcmc = pints.MCMCController(log_posterior, len(xs), xs, method=pints.NoUTurnMCMC)\n",
    "\n",
    "# Add stopping criterion\n",
    "mcmc.set_max_iterations(1000)\n",
    "\n",
    "# Set up modest logging\n",
    "mcmc.set_log_to_screen(True)\n",
    "mcmc.set_log_interval(100)\n",
    "\n",
    "# Run!\n",
    "print('Running...')\n",
    "chains = mcmc.run()\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show traces and histograms\n",
    "pints.plot.trace(chains)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chains have converged!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discard warm up\n",
    "chains = chains[:, 200:]\n",
    "\n",
    "# Check convergence using rhat criterion\n",
    "print('R-hat:')\n",
    "print(pints.rhat_all_params(chains))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract any divergent iterations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
